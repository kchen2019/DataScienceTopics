{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba539c2-7bb1-4622-ad96-46ed188970f8",
   "metadata": {},
   "source": [
    "# This notebook is an example of using XLM roberta model to classify sentiment based on text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd540569-6a09-4728-b59d-dc793a7d1da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install \"tensorflow == 2.8.0\"\n",
    "!pip install \"torch == 1.10.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee7564-c6e6-41c8-936e-89080e8b5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base packages for this task\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6345b5f0-20e3-484a-8673-09cd1ab4288a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install spacy_langdetect\n",
    "!pip install spacy\n",
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a9d68-b5c0-45bd-b4d9-1509f7a2f1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url for loading the dataset\n",
    "url = 'https://www.kaggle.com/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea703b-9940-40ef-afac-cb97861f5ac7",
   "metadata": {},
   "source": [
    "## -------------\n",
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bbadb6-d90f-4904-ba9f-d76ec2d8d9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01f95a-902b-412d-aac6-5007c566e2a5",
   "metadata": {},
   "source": [
    "## ---------------\n",
    "## Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045092c-7f2a-4475-ab51-8acdf905e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language detection \n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "def detect_lan(text) :\n",
    "\n",
    "    doc = nlp(text)\n",
    "    detect_language = doc._.language \n",
    "    detect_language = detect_language['language']\n",
    "\n",
    "    return(detect_language)\n",
    "\n",
    "df['nation'] = df['OriginalTweet'].apply(lambda x: detect_lan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1263aa1-f404-40ad-8f71-454fdfebd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of english samples\n",
    "dict(df.groupby('nation').count()['UserName'])['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e26ae-b111-42be-9a96-7fd85b4e6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of non english samples\n",
    "sum([dict(df.groupby('nation').count()['UserName'])[x] for x in dict(df.groupby('nation').count()['UserName']).keys() if x not in ['en', 'UNKNOWN']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c677b6c-6a82-47c6-ae38-548a8ac14867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the categories and are they balanced?\n",
    "dict(df.groupby('Sentiment').count()['UserName'])['Extremely Negative'] / dict(df.groupby('Sentiment').count()['UserName'])['Positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c19ac-1bec-46ca-949d-78002928c0a0",
   "metadata": {},
   "source": [
    "## ------------------\n",
    "## Text preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45669de-0c9f-496e-9430-df9f09624f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how the text looks like \n",
    "df.OriginalTweet.values[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5e9e4-bd2a-4846-a6bb-5b89240ae9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text processing \n",
    "import re\n",
    "import string\n",
    "def clean_text(text):\n",
    "    text = text.lower() # to lower case \n",
    "    text = re.sub('https:\\/\\/\\S+', '', text) # remove links\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n",
    "    text = re.sub(r'[^ \\w\\.]', '', text) # remove next line \n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # remove words containing numbers\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6612e2-e4bb-4aae-a1f5-823165db4dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df.OriginalTweet.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da4259-8c50-48bd-be30-c4b74e7758c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6c2e4-a663-4e3c-aed3-b3b010795f9f",
   "metadata": {},
   "source": [
    "## --------------------\n",
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63868f56-5249-450b-b42e-ee2724b6cebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"transformers ==4.16.2\"\n",
    "!pip install \"sentencepiece==0.1.96\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307473a-3fea-4ef0-8b5f-c0021a6f43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokens\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Add words into token \n",
    "tokenizer.add_tokens(['covid', 'coronavirus'])\n",
    "\n",
    "print(tokenizer.tokenize('covid'))\n",
    "print(tokenizer.tokenize('coronavirus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79c403-b4c6-4516-b5cc-1d1a06bce822",
   "metadata": {},
   "source": [
    "## --------------------\n",
    "## Tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc275c20-f210-46a3-a945-18dfbee2bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text feature \n",
    "tokenized_feature_raw = tokenizer.batch_encode_plus(\n",
    "                            # Sentences to encode\n",
    "                            df.Text.values.tolist(), \n",
    "                            # Add '[CLS]' and '[SEP]'\n",
    "                            add_special_tokens = True      \n",
    "                   )\n",
    "\n",
    "# collect tokenized sentence length \n",
    "token_sentence_length = [len(x) for x in tokenized_feature_raw['input_ids']]\n",
    "print('max: ', max(token_sentence_length))\n",
    "print('min: ', min(token_sentence_length))\n",
    "\n",
    "# plot the distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.hist(token_sentence_length, rwidth = 0.9)\n",
    "plt.xlabel('Tokenized Sentence Length', fontsize = 18)\n",
    "plt.ylabel('# of Samples', fontsize = 18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801f9ee-e6ea-4455-a8ec-465ac39cfc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify features and target\n",
    "features = df.Text.values.tolist()\n",
    "target = df.Sentiment.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4965f-0dc5-48e0-803a-879652acd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize features \n",
    "MAX_LEN = 128\n",
    "tokenized_feature = tokenizer.batch_encode_plus(\n",
    "                            # Sentences to encode\n",
    "                            features, \n",
    "                            # Add '[CLS]' and '[SEP]'\n",
    "                            add_special_tokens = True,\n",
    "                            # Add empty tokens if len(text)<MAX_LEN\n",
    "                            padding = 'max_length',\n",
    "                            # Truncate all sentences to max length\n",
    "                            truncation=True,\n",
    "                            # Set the maximum length\n",
    "                            max_length = MAX_LEN, \n",
    "                            # Return attention mask\n",
    "                            return_attention_mask = True,\n",
    "                            # Return pytorch tensors\n",
    "                            return_tensors = 'pt'       \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78526413-db7f-433b-abf4-51f7a79b42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d2020-6700-4979-aead-06cc37c7eb82",
   "metadata": {},
   "source": [
    "## --------------------\n",
    "## Train Test split and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ec1f5-5014-4750-a40b-95320f2c1308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fcf785-7d3a-4717-863e-90b657003529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label into numeric \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(target)\n",
    "target_num = le.transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088cb5c-21b8-4b89-8fd3-30bff613ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 80% for training and 20% for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(tokenized_feature['input_ids'], \n",
    "                                                                                                                      target_num,\n",
    "                                                                                                                      tokenized_feature['attention_mask'],\n",
    "                                                                                                      random_state=2018, test_size=0.2, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b69b7ad-012d-4cf3-961a-8f257e3e61e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, torch.tensor(train_labels))\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, torch.tensor(validation_labels))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e063bc-e6a9-4be6-9eea-d7d2a4b8342a",
   "metadata": {},
   "source": [
    "## --------------------\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ff7aa-129b-4574-9bd8-d5b59c7855cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertForSequenceClassification\n",
    "from transformers import XLMRobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\", \n",
    "    # Specify number of classes\n",
    "    num_labels = len(set(target)), \n",
    "    # Whether the model returns attentions weights\n",
    "    output_attentions = False,\n",
    "    # Whether the model returns all hidden-states \n",
    "    output_hidden_states = False\n",
    ")\n",
    "\n",
    "# # tell pytorch to run this model on GPU\n",
    "# model.cuda()\n",
    "\n",
    "# Receive the full size of the new word\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Optimizer & Learning Rate Scheduler\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )\n",
    "\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25210483-7c9c-4e8a-80bf-91499842becd",
   "metadata": {},
   "source": [
    "## --------------------\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07f3fc-d22b-4193-9b89-f0a1a49c444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import time\n",
    "\n",
    "# Store the average loss after each epoch \n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    print('Training on epoch: ', epoch_i)\n",
    "\n",
    "    # set start time \n",
    "    t0 = time.time()\n",
    "\n",
    "    # reset total loss\n",
    "    total_loss = 0\n",
    "\n",
    "    # model in training \n",
    "    model.train()\n",
    "\n",
    "    # loop through batch \n",
    "    # our batch size is 16\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every batch\n",
    "        if step % 16 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "\n",
    "        # clear any previously calculated gradients \n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch)\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "        # get loss\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the norm of the gradients to 1.0.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters \n",
    "        optimizer.step()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaff2de-7cba-4cc5-b624-7f632b06d469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
